{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71226db",
   "metadata": {},
   "source": [
    "# Pytorch運算基本練習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cd6b4",
   "metadata": {},
   "source": [
    "## 熟悉Tensors\n",
    "- Tensors類似矩陣(matrices)或陣列(arrays)。\n",
    "- Tensors可以編碼輸入、輸出或模型參數。\n",
    "- Tensors跟NumPy’s ndarrays很像，差別是Tensors可以在GPUs或其他硬體加速器上跑。\n",
    "- 其實Tensors跟NumPy arrays共用底層記憶體，可以直接橋接(請見[Bridge with NumPy][1])\n",
    "- Tensors也有在自動微分最佳化(接下來在[Autograd][2]我們會講到)\n",
    "\n",
    "[1]: https://docs.pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label \"Tensors跟NumPy arrays共用底層記憶體\" \n",
    "[2]: https://docs.pytorch.org/tutorials/beginner/basics/autograd_tutorial.html \"自動微分\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ff0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6552, 0.9418, 0.4032, 0.3965, 0.0483],\n",
      "        [0.6124, 0.3633, 0.8570, 0.7142, 0.0670],\n",
      "        [0.8184, 0.4314, 0.7275, 0.5116, 0.4181]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(3,5) #維度也可以寫成*[3,5]，代表list展開\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b316b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) #確認可以移動到GPU上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d0b76",
   "metadata": {},
   "source": [
    "### 建立Tensors\n",
    "有很多方式:\n",
    "1. Directly from data\n",
    "2. From a NumPy array\n",
    "3. From another tensor\n",
    "4. By assigning dimensions and filling them with random or constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d8577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors Directly from data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "---------------\n",
      "Tensors From a NumPy array: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "---------------\n",
      "Tensors From another tensor:\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.2017, 0.4208],\n",
      "        [0.6661, 0.9051]]) \n",
      "\n",
      "---------------\n",
      "By assigning dimensions and filling them with random or constant values:\n",
      "Random Tensor: \n",
      " tensor([[0.2258, 0.8503, 0.1571],\n",
      "        [0.9746, 0.5821, 0.5371]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1 Directly from data\n",
    "data = [[1,2], [3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensors Directly from data: \\n {x_data} \\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "# 2 From a NumPy array\n",
    "data_np = np.array([[1,2], [3,4]])\n",
    "x_np = torch.from_numpy(data_np)\n",
    "print(f\"Tensors From a NumPy array: \\n {x_np} \\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "# 3 From another tensor\n",
    "print(\"Tensors From another tensor:\")\n",
    "x_ones = torch.ones_like(x_data) #保留原tensors properties (shape, datatype)，內容全填1\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) #原tensors的datatype改成float\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "# 4 Assigning dimensions\n",
    "shape = (2, 3,) #tuple只有一個元素的時候一定要加上逗號\n",
    "rand_tensor = torch.rand(2,3) #參數可以是int sequence或是傳入tuple, list都可\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(\"By assigning dimensions and filling them with random or constant values:\")\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce2734",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "包含shape, datatype, and the device they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c200bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "張量的shape: torch.Size([2, 3])\n",
      "張量的datatype: torch.float32\n",
      "張量存放的位置: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2,3)\n",
    "print(f\"張量的shape: {tensor.shape}\")\n",
    "print(f\"張量的datatype: {tensor.dtype}\")\n",
    "print(f\"張量存放的位置: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3d370",
   "metadata": {},
   "source": [
    "### 把tensor移動到GPU\n",
    "tensor提供超過100種運算，包括: 算術、線性代數、矩陣運算（轉置、索引、切片）、取樣[等][1]\n",
    "\n",
    "[1]: https://pytorch.org/docs/stable/torch.html \"torch docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2e1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "張量存放的位置: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda') #一定要=，因為tensor.to()是回傳一份新的tensor，不是參照\n",
    "print(f\"張量存放的位置: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e07d19",
   "metadata": {},
   "source": [
    "### Numpy-like的索引和切片\n",
    "`a:b`代表索引a~b-1  \n",
    "`a:b:c`代表索引a, a+c, a+2c 一直到超過b就停止(不包含b，如果c是正的，最多到b-1；如果c是負的，最多到b+1)  \n",
    "`:`代表該維度的全部索引  \n",
    "`...`代表剩餘全部維度全部索引，可以放在開頭、中間、結尾，**但只能出現一次**  \n",
    "`tensor[0]`如果索引在開頭，可以不寫後面的索引，代表選取剩餘全部維度全部索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090b0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Batch: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "First Batch -> Second Row: \n",
      " tensor([1., 1., 1., 1.]) \n",
      "\n",
      "Last Column: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "After assign 0 to second column: \n",
      " tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(2,3,4)\n",
    "print(f\"First Batch: \\n {tensor[0]} \\n\") #或是tensor[0,:,:] 或tensor[0,...]\n",
    "print(f\"First Batch -> Second Row: \\n {tensor[0,1,:]} \\n\") #或是tensor[0,1]\n",
    "print(f\"Last Column: \\n {tensor[..., -1]} \\n\") #或是tensor[:,:,-1]\n",
    "tensor[...,1] = 0\n",
    "print(f\"After assign 0 to second column: \\n {tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b683279",
   "metadata": {},
   "source": [
    "### 連接tensors\n",
    "**torch.cat 是「連接」，torch.stack 是「堆疊」。**\n",
    "\n",
    "`torch.cat([tensor, tensor, tensor], dim=)`\n",
    "> torch.cat() 用來「沿著指定的維度，把多個 tensor 串接起來（concatenate）」，前提是要串接的 tensor 在非指定維度上形狀要一致，**不會增加新的維度，但會增加指定維度的元素個數**。這邊傳入`tensor1`, `tensor2`, `tensor3`，3個tensor就會在指定維度上合併所有tensor的元素，假設大家都是2x2，`dim=1`就會變成2x6，`dim=0`就會變成6x2。\n",
    "\n",
    "`torch.stack([tensor1, tensor2, tensor3], dim=)`\n",
    "> torch.stack() 用來將**多個 shape 完全一樣的 tensor 沿著一個「新維度」堆疊起來，會增加一個新的維度在指定`dim`**。這邊傳入`tensor1`, `tensor2`, `tensor3`，3個tensor就會在指定維度上新增3個數量，假設大家都是2x2，`dim=1`就會變成2x3x2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339f260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor2:\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor3:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "===cat===\n",
      "torch.cat([tensor1, tensor2, tensor3], dim=1):\n",
      "tensor([[1., 1., 2., 2., 3., 3.],\n",
      "        [1., 1., 2., 2., 3., 3.]])\n",
      "shape: torch.Size([2, 6])\n",
      "---\n",
      "torch.cat([tensor1, tensor2, tensor3], dim=0):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]])\n",
      "shape: torch.Size([6, 2])\n",
      "=========\n",
      "\n",
      "===stack===\n",
      "torch.stack([tensor1, tensor2, tensor3], dim=1):\n",
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]]])\n",
      "shape: torch.Size([2, 3, 2])\n",
      "---\n",
      "torch.stack([tensor1, tensor2, tensor3], dim=0):\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [3., 3.]]])\n",
      "shape: torch.Size([3, 2, 2])\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones(2,2)\n",
    "tensor2 = torch.full_like(tensor1, 2) #建立都是2的矩陣，維度和tensor1一樣\n",
    "tensor3 = torch.full_like(tensor1, 3) #建立都是3的矩陣，維度和tensor1一樣\n",
    "\n",
    "print(f\"tensor1:\\n{tensor1}\")\n",
    "print(f\"tensor2:\\n{tensor2}\")\n",
    "print(f\"tensor3:\\n{tensor3}\")\n",
    "\n",
    "print(\"===cat===\")\n",
    "tensor_cat = torch.cat([tensor1, tensor2, tensor3], dim=1)\n",
    "print(f\"torch.cat([tensor1, tensor2, tensor3], dim=1):\\n{tensor_cat}\")\n",
    "print(f\"shape: {tensor_cat.shape}\")\n",
    "print(\"---\")\n",
    "tensor_cat = torch.cat([tensor1, tensor2, tensor3], dim=0)\n",
    "print(f\"torch.cat([tensor1, tensor2, tensor3], dim=0):\\n{tensor_cat}\")\n",
    "print(f\"shape: {tensor_cat.shape}\")\n",
    "print(\"=========\\n\")\n",
    "\n",
    "print(\"===stack===\")\n",
    "tensor_stack = torch.stack([tensor1, tensor2, tensor3], dim=1)\n",
    "print(f\"torch.stack([tensor1, tensor2, tensor3], dim=1):\\n{tensor_stack}\")\n",
    "print(f\"shape: {tensor_stack.shape}\")\n",
    "print(\"---\")\n",
    "tensor_stack = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
    "print(f\"torch.stack([tensor1, tensor2, tensor3], dim=0):\\n{tensor_stack}\")\n",
    "print(f\"shape: {tensor_stack.shape}\")\n",
    "print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03181f8",
   "metadata": {},
   "source": [
    "### 數學運算(Arithmetic operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53268a",
   "metadata": {},
   "source": [
    "#### 矩陣乘法(Matrix Multiplication)\n",
    "A @ B: A的最後一個維度(column)size要跟B倒數第二個維度(row)一樣，前面其他維度也要一樣，例如batch=10。例: A(10,2,3) @ B(10,3,4) = C(10,2,4)\n",
    ">Tips: 只要看最後兩個維度，前面所有維度當作batch，batch要一樣。\n",
    "\n",
    "用法: `@` 或 `tensor.matmul()` 或 `torch.matmul()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe201219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor2:\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor2.T:\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "===Matrix Multiplication===\n",
      "tensor1 @ tensor2.T:\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor1.matmul(tensor2.T):\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "torch.matmul(tensor1, tensor2.T, out=y3):\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.full([2,3], 1.0)\n",
    "tensor2 = torch.full([2,3], 2.0)\n",
    "\n",
    "print(f\"tensor1:\\n{tensor1}\")\n",
    "print(f\"tensor2:\\n{tensor2}\")\n",
    "print(f\"tensor2.T:\\n{tensor2.T}\")\n",
    "\n",
    "print(\"===Matrix Multiplication===\")\n",
    "y1 = tensor1 @ tensor2.T\n",
    "print(f\"tensor1 @ tensor2.T:\\n{y1}\")\n",
    "y2 = tensor1.matmul(tensor2.T)\n",
    "print(f\"tensor1.matmul(tensor2.T):\\n{y2}\")\n",
    "# 先創造一個可以儲存結果的矩陣。\n",
    "# tensor1.shape是2x3，tensor2.T.shape是3x2，兩個矩陣相乘後會變成2x2，也就是取tensor1.shape[0]xtensor2.T.shape[1]\n",
    "y3 = torch.rand([tensor1.shape[0], tensor2.T.shape[1]])\n",
    "torch.matmul(tensor1, tensor2.T, out=y3) #儲存結果到y3\n",
    "print(f\"torch.matmul(tensor1, tensor2.T, out=y3):\\n{y3}\")\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68655a",
   "metadata": {},
   "source": [
    "#### 逐元素乘法(Element-wise Multiplication)\n",
    "shape 必須完全一樣\n",
    "\n",
    "語法: `*` 或 `tensor.mul()` 或 `torch.mul()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eacef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor2:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "===Element-wise Multiplication===\n",
      "tensor1 * tensor2:\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor1.mul(tensor2):\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "torch.mul(tensor1, tensor2, out=z3):\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.full([2,2], 2.0)\n",
    "tensor2 = torch.full([2,2], 3.0)\n",
    "\n",
    "print(f\"tensor1:\\n{tensor1}\")\n",
    "print(f\"tensor2:\\n{tensor2}\")\n",
    "\n",
    "print(\"===Element-wise Multiplication===\")\n",
    "z1 = tensor1 * tensor2\n",
    "print(f\"tensor1 * tensor2:\\n{z1}\")\n",
    "z2 = tensor1.mul(tensor2)\n",
    "print(f\"tensor1.mul(tensor2):\\n{z2}\")\n",
    "z3 = torch.rand_like(tensor1) #z3用來儲存結果\n",
    "torch.mul(tensor1, tensor2, out=z3)\n",
    "print(f\"torch.mul(tensor1, tensor2, out=z3):\\n{z3}\")\n",
    "print(\"======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10b843",
   "metadata": {},
   "source": [
    "### 一個元素的tensor轉型成python的數值型別(float或int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac4d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "agg before item(): 4.0, type: <class 'torch.Tensor'>, dtype: torch.float32\n",
      "agg after item(): 4.0, type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones([2,2])\n",
    "print(f\"tensor:\\n{tensor}\")\n",
    "agg = tensor.sum()\n",
    "print(f\"agg before item(): {agg}, type: {type(agg)}, dtype: {agg.dtype}\")\n",
    "agg_item = agg.item()\n",
    "print(f\"agg after item(): {agg_item}, type: {type(agg_item)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d01da",
   "metadata": {},
   "source": [
    "### 就地操作(In-place operations)\n",
    "語法: 物件方法名稱後綴加上`_`或是`torch.函數名稱()`加上參數`out=變數名稱`。  \n",
    "In-place operation：會直接改變原本的 tensor 值，而不是建立一個新的 tensor。  \n",
    "> 可節省記憶體，但是不利於自動微分，因為歷史的計算圖會遺失，導致 `.backward()` 會出錯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9818d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "tensor after .add(3):\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "tensor after .add_(5):\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "\n",
      "torch.add(tensor, 5, out=tensor):\n",
      "tensor([[10., 10.],\n",
      "        [10., 10.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros([2,2])\n",
    "print(f\"tensor:\\n{tensor}\\n\")\n",
    "tensor.add(3) #要寫成tensor = tensor.add(3)，因為是回傳新的值，不是inplace\n",
    "print(f\"tensor after .add(3):\\n{tensor}\\n\")\n",
    "tensor.add_(5) #inplace 物件方法風格\n",
    "print(f\"tensor after .add_(5):\\n{tensor}\\n\")\n",
    "torch.add(tensor, 5, out=tensor) #inplace 函數風格\n",
    "print(f\"torch.add(tensor, 5, out=tensor):\\n{tensor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de67be",
   "metadata": {},
   "source": [
    "### Bridge with NumPy\n",
    "在CPU上的Tensors可以跟NumPy arrays共享底層記憶體位置，改變其中一個另一個也會跟著改變。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012d214",
   "metadata": {},
   "source": [
    "#### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed9e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "n:\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones([2,2])\n",
    "print(f\"t:\\n{t}\")\n",
    "\n",
    "n = t.numpy()\n",
    "print(f\"n:\\n{n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060f2ca",
   "metadata": {},
   "source": [
    "##### Tensors會改變Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "968a3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "n:\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "t.add_(2) #要inplace才會改變numpy\n",
    "print(f\"t:\\n{t}\")\n",
    "print(f\"n:\\n{n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8688f1",
   "metadata": {},
   "source": [
    "#### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40596763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "t:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = np.ones([2,2])\n",
    "print(f\"n:\\n{n}\")\n",
    "\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t:\\n{t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190ed4d",
   "metadata": {},
   "source": [
    "##### Numpy arrays會改變Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd0a702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n:\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "t:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 2, out=n) #要inplace操作才會改變numpy及tensor的同個記憶體位置裡面的值\n",
    "print(f\"n:\\n{n}\")\n",
    "print(f\"t:\\n{t}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THUIR-COLIEE2023-WSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
